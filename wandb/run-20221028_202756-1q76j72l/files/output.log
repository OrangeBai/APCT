Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
Missing logger folder: /home/orange/Main/Experiment/ICLR/cifar10/vgg16_express_0/lightning_logs
/home/orange/Programs/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /home/orange/Main/Experiment/ICLR/cifar10/vgg16_express_0 exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Files already downloaded and verified
Files already downloaded and verified
Sanity Checking: 0it [00:00, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name          | Type             | Params
---------------------------------------------------
0 | model         | VGG              | 33.7 M
1 | attack        | Vanilla          | 33.7 M
2 | loss_function | CrossEntropyLoss | 0
---------------------------------------------------
33.7 M    Trainable params
0         Non-trainable params
33.7 M    Total params

Epoch 0:   0%|▍                                                                                                  | 2/468 [00:00<03:50,  2.02it/s, loss=2.51, v_num=0]
/home/orange/Programs/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate





















































































Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:59<00:00,  7.88it/s, loss=1.82, v_num=0]
`Trainer.fit` stopped: `max_epochs=1` reached.